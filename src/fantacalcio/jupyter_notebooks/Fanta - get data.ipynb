{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INDICES=['V', 'G', 'A', 'R', 'RS', 'AG', 'AM', 'ES', 'FV']\n",
    "\n",
    "# page_votes = \"https://www.fantagazzetta.com/voti-serie-a/2016-17/\"\n",
    "# page_votes = \"http://www.gazzetta.it/calcio/fantanews/voti/serie-a-2016-17/giornata-\"\n",
    "pages_grades = {'2014':\"http://www.gazzetta.it/calcio/fantanews/voti/serie-a-2014-15/giornata-\",\n",
    "                '2015':\"http://www.gazzetta.it/calcio/fantanews/voti/serie-a-2015-16/giornata-\",\n",
    "                '2016':\"http://www.gazzetta.it/calcio/fantanews/voti/serie-a-2016-17/giornata-\",\n",
    "               }\n",
    "n_giornate = 38\n",
    "\n",
    "\n",
    "\n",
    "folder = \"C:\\\\Users\\\\zuk-8\\\\Programs\\\\workspace\\\\jupiter_notebooks\\\\Andrea\"\n",
    "fn = \"fanta_grades.csv\"\n",
    "link_fn = \"player_links.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_table_cells(str):\n",
    "    str = str.replace('\\r', '').replace('\\n', '').replace('\\t', '').replace(' ', '')\n",
    "    try:\n",
    "        return float(str)\n",
    "    except ValueError:\n",
    "        return pd.np.NaN\n",
    "\n",
    "def get_team_soups(page_votes, giornata):\n",
    "    # Get html data from page+giornata\n",
    "    page = page_votes+str(giornata)\n",
    "    r = requests.get(page)\n",
    "    data = r.text\n",
    "    # Navigate inside data using BS\n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    "    soup1 = [sec for sec in soup.html.body.find_all('section') if sec.get('class', None)==['main-container']][0]\n",
    "    soup2 = [sec for sec in soup1.find_all('section') if sec.get('class', None)==['section-standard-row']][0]\n",
    "    soup3 = [sec for sec in soup2.find_all('div') if sec.get('class', None)==[\"MXXX-central-articles-main-column\"]][0]\n",
    "    soup4 = [sec for sec in soup3.find_all('div') if\n",
    "                   sec.get('class', None)==['magicDayList', 'listView', 'magicDayListChkDay']][0]\n",
    "    return [sec for sec in soup4.find_all('div') if sec.get('class', None)==['singleRound']]\n",
    "\n",
    "def get_player_soup(team_soup):\n",
    "    team_list = [sec for sec in team_soup.find_all('div') if sec.get('class', None)==[\"magicTeamListContainer\"]][0]\n",
    "    team_grades = [sec for sec in team_list.find_all('ul') if sec.get('class')==[\"magicTeamList\"]][0]\n",
    "    team = [span.contents[0] for span in team_grades.li.div.find_all('span') if span.get('class', None)==[\"teamNameIn\"]][0]\n",
    "    player_soups = team_grades.find_all('li')[1:]\n",
    "    return team, player_soups\n",
    "\n",
    "def get_player_name(player_soup):\n",
    "    return [span.string for span in player_soup.div.div.find_all('span') if span.get('class',None)==[\"playerNameIn\"]][0]\n",
    "\n",
    "def get_player_link(player_soup):\n",
    "    return [a.get('href') for a in player_soup.find_all('a') if a.get('href', None)!=None][0]\n",
    "\n",
    "def get_player_stats(player_soup, indices=INDICES):\n",
    "    player_name = get_player_name(player_soup)\n",
    "    role = [span.string for span in player_soup.div.find_all('span') if\n",
    "            span.get('class',None)==[\"playerRole\", \"show-for-small\"]][0]\n",
    "    stats = pd.Series(data=[clean_table_cells(div.string)\n",
    "            for div in player_soup.find_all('div') if div.get('class', None)[0] == \"inParameter\"],\n",
    "      index=indices)\n",
    "    player_in = [sp for sp in player_soup.find_all('span') if sp.get('class',None)==['playerStats', 'icon', 'down']]\n",
    "    player_out = [sp for sp in player_soup.find_all('span') if sp.get('class',None)==['playerStats', 'icon', 'up']]\n",
    "    pi = 1 if len(player_in)>0 else 0\n",
    "    po = 1 if len(player_out)>0 else 0\n",
    "    stats = stats.append(pd.Series(index=['In', 'Out'], data = [pi, po]))\n",
    "    return player_name, role, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_stats = pd.DataFrame()\n",
    "for season,page_grades in pages_grades.items():\n",
    "    season_stats = pd.DataFrame()\n",
    "    for giornata in range(1,n_giornate+1):\n",
    "        team_stats = pd.DataFrame()\n",
    "        for team_soup in get_team_soups(page_grades, giornata):\n",
    "            team, player_soups = get_player_soup(team_soup)\n",
    "            player_stats = pd.DataFrame()\n",
    "            for player_soup in player_soups:\n",
    "                player_name, role, stats = get_player_stats(player_soup, indices=INDICES)\n",
    "                ps = pd.DataFrame({player_name:stats}).T\n",
    "                ps['Role'] = role\n",
    "                player_stats = pd.concat([player_stats, ps])\n",
    "            player_stats = player_stats.reset_index().rename(columns={'index':'Player'})\n",
    "            player_stats['Team'] = team\n",
    "            team_stats = pd.concat([team_stats, player_stats])\n",
    "        team_stats['Giornata'] = str(giornata)\n",
    "        season_stats = pd.concat([season_stats, team_stats])\n",
    "    season_stats['Season'] = season\n",
    "    all_stats = pd.concat([all_stats, season_stats])\n",
    "all_stats = all_stats.set_index(['Season', 'Giornata', 'Team', 'Player'])\n",
    "all_stats.to_csv(os.path.join(folder, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_links = dict()\n",
    "for season,page_grades in pages_grades.items():\n",
    "    for giornata in range(1,n_giornate+1):\n",
    "        for team_soup in get_team_soups(page_grades, giornata):\n",
    "            team, player_soups = get_player_soup(team_soup)\n",
    "            for player_soup in player_soups:\n",
    "                all_links[(get_player_name(player_soup), season)] = get_player_link(player_soup)\n",
    "\n",
    "all_links_df = pd.DataFrame(columns=['Player', 'Season', 'Link'])\n",
    "for k,v in all_links.items():\n",
    "    all_links_df = all_links_df.append(pd.DataFrame(data=[[k[0], k[1], v]], columns=['Player', 'Season', 'Link']))\n",
    "all_links_df = all_links_df.set_index(['Season', 'Player']).sort_index().reset_index()\n",
    "all_links_df.to_csv(os.path.join(folder, link_fn), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attribute_from_text(str):\n",
    "    return clean_attributes(str.replace('\\r', '').replace('\\n', '').replace('\\t', '').replace('  ', '').replace(' ', '_')).\\\n",
    "         split(':')\n",
    "\n",
    "def clean_attributes(str):\n",
    "    return str.replace('_cm', '').replace('_kg', '')\n",
    "\n",
    "def get_attribute_from_player_soup(player_soup):\n",
    "    # Navigate towards the place where information is stored\n",
    "    main_container = [sec for sec in player_soup.body.find_all('section') if sec.get('class',None)==['main-container']][0]\n",
    "    opener = [sec for sec in main_container.find_all('section') if sec.get('class',None)==['opener']][0]\n",
    "    opener2 = [div for div in opener.find_all('div') if div.get('class',None)==['MXXX-section-opener-column']][0]\n",
    "    opener3 = [sec for sec in opener2.find_all('section') if sec.get('class',None)==['profilo-giocatore']][0]\n",
    "    profile = [div for div in opener3.find_all('div') if div.get('class',None)==[\"first_half_profilo\"]][0]\n",
    "    profile_data = [div for div in profile.find_all('div') if div.get('class',None)==[\"right\"]][0]\n",
    "    # Get the data and store it in a df\n",
    "    data=[get_attribute_from_text(p.get_text()) for p in profile_data.find_all('p')]\n",
    "    return pd.DataFrame(data=[[d[1] for d in data]], columns=[d[0] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_links_df = pd.read_csv(os.path.join(folder, link_fn))\n",
    "columns = ['Player', 'Season', 'Link', 'Ruolo', 'Squadra', 'Data_di_nascita', 'Nazionalita', 'Altezza', 'Peso']\n",
    "player_attributes = pd.DataFrame(columns=columns)\n",
    "for row in all_links_df.index:\n",
    "    player_name = all_links_df.loc[row]['Player']\n",
    "    player_season = all_links_df.loc[row]['Season']\n",
    "    player_link = all_links_df.loc[row]['Link']\n",
    "    player_soup = BeautifulSoup(requests.get(player_link).text, \"lxml\")\n",
    "    player_data = get_attribute_from_player_soup(player_soup)\n",
    "    player_data['Player'] = player_name\n",
    "    player_data['Season'] = player_season\n",
    "    player_data['Link'] = player_link\n",
    "    player_attributes = player_attributes.append(player_data)\n",
    "player_attributes[columns].to_csv(os.path.join(folder, attr_fn), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Get page links\n",
    "\n",
    "# page = \"//www.fantagazzetta.com/squadre\"\n",
    "# player_teams = {}\n",
    "# player_links = {}\n",
    "\n",
    "# teams = ['Atalanta', 'Bologna', 'Cagliari', 'Chievo', 'Crotone', 'Empoli', 'Fiorentina',\n",
    "#          'Genoa', 'Inter', 'Juventus', 'Lazio', 'Milan', 'Napoli', 'Palermo', 'Pescara',\n",
    "#          'Roma', 'Sampdoria', 'Sassuolo', 'Torino', 'Udinese']\n",
    "\n",
    "# for team in teams:\n",
    "#     site = page+\"/\"+team\n",
    "#     r  = requests.get(\"https:\"+site)\n",
    "#     data = r.text\n",
    "#     soup = BeautifulSoup(data, \"lxml\")\n",
    "#     for link in soup.find_all('a'):\n",
    "#         l = link.get('href', '')\n",
    "#         if l.startswith(site):\n",
    "#             player_teams[link.contents[0]] = team\n",
    "#             player_links[link.contents[0]] = l\n",
    "\n",
    "# for player,link in list(player_links.items())[1:2]:\n",
    "#     site = link + \"/2/2016-17\"\n",
    "    \n",
    "#     r  = requests.get(\"https:\"+site)\n",
    "#     data = r.text\n",
    "#     soup = BeautifulSoup(data, \"html.parser\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:standard]",
   "language": "python",
   "name": "conda-env-standard-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
